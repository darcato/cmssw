diff --git a/.gitignore b/.gitignore
index 82c501e6c41..2c00cf3f04b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,3 +4,4 @@ __init__.py
 .#*
 #*#
 *~
+.vscode/c_cpp_properties.json
diff --git a/CondFormats/SiPixelObjects/interface/SiPixelGainForHLTonGPU.h b/CondFormats/SiPixelObjects/interface/SiPixelGainForHLTonGPU.h
index 931ee7e65f2..fcd7e90eaf0 100644
--- a/CondFormats/SiPixelObjects/interface/SiPixelGainForHLTonGPU.h
+++ b/CondFormats/SiPixelObjects/interface/SiPixelGainForHLTonGPU.h
@@ -6,6 +6,7 @@
 #include <tuple>
 
 #include "HeterogeneousCore/CUDAUtilities/interface/cuda_assert.h"
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
 
 struct SiPixelGainForHLTonGPU_DecodingStructure{
   uint8_t gain;
@@ -23,7 +24,7 @@ class SiPixelGainForHLTonGPU {
   using Range = std::pair<uint32_t,uint32_t>;
  
 
-  inline __host__ __device__
+  inline ALPAKA_FN_ACC
   std::pair<float,float> getPedAndGain(uint32_t moduleInd, int col, int row, bool& isDeadColumn, bool& isNoisyColumn ) const {
 
 
diff --git a/DataFormats/BeamSpot/interface/BeamSpot.h b/DataFormats/BeamSpot/interface/BeamSpot.h
index 7afef86f3a4..defb2d14bae 100755
--- a/DataFormats/BeamSpot/interface/BeamSpot.h
+++ b/DataFormats/BeamSpot/interface/BeamSpot.h
@@ -29,8 +29,8 @@ namespace reco {
     typedef math::XYZPoint Point;
     enum { dimension = 7 };
     typedef math::Error<dimension>::type CovarianceMatrix;
-    enum { dim3 = 3 };
-    typedef math::Error<dim3>::type Covariance3DMatrix;
+    enum { dimension3 = 3 };
+    typedef math::Error<dimension3>::type Covariance3DMatrix;
     enum { resdim = 2 };
     typedef math::Error<resdim>::type ResCovMatrix;
 	
diff --git a/HeterogeneousCore/CUDAServices/BuildFile.xml b/HeterogeneousCore/CUDAServices/BuildFile.xml
index 61572b96fb2..993ecad4aee 100644
--- a/HeterogeneousCore/CUDAServices/BuildFile.xml
+++ b/HeterogeneousCore/CUDAServices/BuildFile.xml
@@ -6,6 +6,7 @@
 <use name="cuda"/>
 <use name="cuda-api-wrappers"/>
 <use name="cub"/>
+<flags CXXFLAGS="-g -I/data/user/dmarcato/cupla/include/ -I/data/user/dmarcato/alpaka/include"/>
 
 <export>
     <lib name="1"/>
diff --git a/HeterogeneousCore/CUDAServices/src/CUDAService.cc b/HeterogeneousCore/CUDAServices/src/CUDAService.cc
index b2c412ee379..5a9f3be672e 100644
--- a/HeterogeneousCore/CUDAServices/src/CUDAService.cc
+++ b/HeterogeneousCore/CUDAServices/src/CUDAService.cc
@@ -2,7 +2,7 @@
 #include <iostream>
 #include <limits>
 
-#include <cuda.h>
+//#include <cuda.h>
 #include <cuda/api_wrappers.h>
 
 #include "FWCore/MessageLogger/interface/MessageLogger.h"
@@ -447,22 +447,22 @@ void *CUDAService::allocate_device(int dev, size_t nbytes, cuda::stream_t<>& str
       throw std::runtime_error("Tried to allocate "+std::to_string(nbytes)+" bytes, but the allocator maximum is "+std::to_string(allocator_->maxAllocation));
     }
 
-    cuda::throw_if_error(allocator_->deviceAllocator.DeviceAllocate(dev, &ptr, nbytes, stream.id()));
+    throw_if_error(allocator_->deviceAllocator.DeviceAllocate(dev, &ptr, nbytes, stream.id()));
   }
   else {
     cuda::device::current::scoped_override_t<> setDeviceForThisScope(dev);
-    cuda::throw_if_error(cudaMalloc(&ptr, nbytes));
+    throw_if_error(cudaMalloc(&ptr, nbytes));
   }
   return ptr;
 }
 
 void CUDAService::free_device(int device, void *ptr) {
   if(allocator_) {
-    cuda::throw_if_error(allocator_->deviceAllocator.DeviceFree(device, ptr));
+    throw_if_error(allocator_->deviceAllocator.DeviceFree(device, ptr));
   }
   else {
     cuda::device::current::scoped_override_t<> setDeviceForThisScope(device);
-    cuda::throw_if_error(cudaFree(ptr));
+    throw_if_error(cudaFree(ptr));
   }
 }
 
@@ -474,10 +474,10 @@ void *CUDAService::allocate_host(size_t nbytes, cuda::stream_t<>& stream) {
       throw std::runtime_error("Tried to allocate "+std::to_string(nbytes)+" bytes, but the allocator maximum is "+std::to_string(allocator_->maxAllocation));
     }
 
-    cuda::throw_if_error(allocator_->hostAllocator.HostAllocate(&ptr, nbytes, stream.id()));
+    throw_if_error(allocator_->hostAllocator.HostAllocate(&ptr, nbytes, stream.id()));
   }
   else {
-    cuda::throw_if_error(cudaMallocHost(&ptr, nbytes));
+    throw_if_error(cudaMallocHost(&ptr, nbytes));
   }
 
   return ptr;
@@ -485,10 +485,10 @@ void *CUDAService::allocate_host(size_t nbytes, cuda::stream_t<>& stream) {
 
 void CUDAService::free_host(void *ptr) {
   if(allocator_) {
-    cuda::throw_if_error(allocator_->hostAllocator.HostFree(ptr));
+    throw_if_error(allocator_->hostAllocator.HostFree(ptr));
   }
   else {
-    cuda::throw_if_error(cudaFreeHost(ptr));
+    throw_if_error(cudaFreeHost(ptr));
   }
 }
 
diff --git a/HeterogeneousCore/CUDAUtilities/interface/CUDAHostAllocator.h b/HeterogeneousCore/CUDAUtilities/interface/CUDAHostAllocator.h
index 68cf8074991..e9300333164 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/CUDAHostAllocator.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/CUDAHostAllocator.h
@@ -34,7 +34,7 @@ public:
   T* allocate(std::size_t n) const __attribute__((warn_unused_result)) __attribute__((malloc)) __attribute__((returns_nonnull))
   {
     void* ptr = nullptr;
-    cudaError_t status = cudaMallocHost(&ptr, n * sizeof(T), FLAGS);
+    cudaError_t status = cudaHostAlloc(&ptr, n * sizeof(T), FLAGS);
     if (status != cudaSuccess) {
       throw cuda_bad_alloc(status);
     }
diff --git a/HeterogeneousCore/CUDAUtilities/interface/GPUSimpleVector.h b/HeterogeneousCore/CUDAUtilities/interface/GPUSimpleVector.h
index 0a8df34b512..e251834616e 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/GPUSimpleVector.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/GPUSimpleVector.h
@@ -7,6 +7,8 @@
 #include <utility>
 
 #include "HeterogeneousCore/CUDAUtilities/interface/cudaCompat.h"
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
+
 
 namespace GPU {
   template <class T>
@@ -45,9 +47,9 @@ namespace GPU {
       }
     }
 
-    __device__ inline T &back() { return m_data[m_size - 1]; }
+    ALPAKA_FN_ACC inline T &back() { return m_data[m_size - 1]; }
 
-    __device__ inline const T &back() const {
+    ALPAKA_FN_ACC inline const T &back() const {
       if (m_size > 0) {
         return m_data[m_size - 1];
       } else
@@ -55,7 +57,8 @@ namespace GPU {
     }
 
     // thread-safe version of the vector, when used in a CUDA kernel
-    __device__ int push_back(const T &element) {
+    template<typename T_Acc> 
+    ALPAKA_FN_ACC int push_back(T_Acc const & acc, const T &element) {
       auto previousSize = atomicAdd(&m_size, 1);
       if (previousSize < m_capacity) {
         m_data[previousSize] = element;
@@ -66,8 +69,8 @@ namespace GPU {
       }
     }
 
-    template <class... Ts>
-    __device__ int emplace_back(Ts &&... args) {
+    template <typename T_Acc, class... Ts>
+    ALPAKA_FN_ACC int emplace_back(T_Acc const & acc, Ts &&... args) {
       auto previousSize = atomicAdd(&m_size, 1);
       if (previousSize < m_capacity) {
         (new (&m_data[previousSize]) T(std::forward<Ts>(args)...));
@@ -79,7 +82,8 @@ namespace GPU {
     }
 
     // thread safe version of resize
-    __device__ int extend(int size = 1) {
+    template<typename T_Acc> 
+    ALPAKA_FN_ACC int extend(T_Acc const & acc, int size = 1) {
       auto previousSize = atomicAdd(&m_size, size);
       if (previousSize < m_capacity) {
         return previousSize;
@@ -89,7 +93,8 @@ namespace GPU {
       }
     }
 
-    __device__ int shrink(int size = 1) {
+    template<typename T_Acc> 
+    ALPAKA_FN_ACC  int shrink(T_Acc const & acc, int size = 1) {
       auto previousSize = atomicSub(&m_size, size);
       if (previousSize >= size) {
         return previousSize - size;
diff --git a/HeterogeneousCore/CUDAUtilities/interface/HistoContainer.h b/HeterogeneousCore/CUDAUtilities/interface/HistoContainer.h
index 2584ae7a4e0..3902e493194 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/HistoContainer.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/HistoContainer.h
@@ -22,37 +22,47 @@
 namespace cudautils {
 
   template <typename Histo, typename T>
-  __global__ void countFromVector(Histo *__restrict__ h,
+  struct countFromVector
+  {
+    template< typename T_Acc >
+   ALPAKA_FN_ACC
+    void operator()( T_Acc const & acc, Histo *__restrict__ h,
                                   uint32_t nh,
                                   T const *__restrict__ v,
-                                  uint32_t const *__restrict__ offsets) {
-    int first = blockDim.x * blockIdx.x + threadIdx.x;
-    for (int i = first; i < offsets[nh]; i += gridDim.x * blockDim.x) {
-      auto off = cuda_std::upper_bound(offsets, offsets + nh + 1, i);
-      assert((*off) > 0);
-      int32_t ih = off - offsets - 1;
-      assert(ih >= 0);
-      assert(ih < nh);
-      (*h).count(v[i], ih);
+                                  uint32_t const *__restrict__ offsets) const {
+      int first = blockDim.x * blockIdx.x + threadIdx.x;
+      for (uint32_t i = first; i < offsets[nh]; i += gridDim.x * blockDim.x) {
+        auto off = cuda_std::upper_bound(offsets, offsets + nh + 1, i);
+        assert((*off) > 0);
+        int32_t ih = off - offsets - 1;
+        assert(ih >= 0);
+        assert(ih < nh);
+        (*h).count(v[i], ih);
+      }
     }
-  }
+  };
 
   template <typename Histo, typename T>
-  __global__ void fillFromVector(Histo *__restrict__ h,
+  struct fillFromVector
+  {
+    template< typename T_Acc >
+   ALPAKA_FN_ACC
+    void operator()( T_Acc const & acc, Histo *__restrict__ h,
                                  uint32_t nh,
                                  T const *__restrict__ v,
-                                 uint32_t const *__restrict__ offsets) {
-    int first = blockDim.x * blockIdx.x + threadIdx.x;
-    for (int i = first; i < offsets[nh]; i += gridDim.x * blockDim.x) {
-      auto off = cuda_std::upper_bound(offsets, offsets + nh + 1, i);
-      assert((*off) > 0);
-      int32_t ih = off - offsets - 1;
-      assert(ih >= 0);
-      assert(ih < nh);
-      (*h).fill(v[i], i, ih);
+                                 uint32_t const *__restrict__ offsets) const {
+      int first = blockDim.x * blockIdx.x + threadIdx.x;
+      for (uint32_t i = first; i < offsets[nh]; i += gridDim.x * blockDim.x) {
+        auto off = cuda_std::upper_bound(offsets, offsets + nh + 1, i);
+        assert((*off) > 0);
+        int32_t ih = off - offsets - 1;
+        assert(ih >= 0);
+        assert(ih < nh);
+        (*h).fill(v[i], i, ih);
+      }
     }
-  }
-
+  };
+  
   template <typename Histo>
   void launchZero(Histo *__restrict__ h,
                   cudaStream_t stream
@@ -120,15 +130,20 @@ namespace cudautils {
   }
 
   template <typename Assoc>
-  __global__ void finalizeBulk(AtomicPairCounter const *apc, Assoc *__restrict__ assoc) {
-    assoc->bulkFinalizeFill(*apc);
-  }
+  struct finalizeBulk
+  {
+    template< typename T_Acc >
+   ALPAKA_FN_ACC
+    void operator()( T_Acc const & acc, AtomicPairCounter const *apc, Assoc *__restrict__ assoc ) const {
+      assoc->bulkFinalizeFill(acc, *apc);
+    }
+  };
 
 }  // namespace cudautils
 
 // iteratate over N bins left and right of the one containing "v"
 template <typename Hist, typename V, typename Func>
-__host__ __device__ __forceinline__ void forEachInBins(Hist const &hist, V value, int n, Func func) {
+ALPAKA_FN_ACC __forceinline__ void forEachInBins(Hist const &hist, V value, int n, Func func) {
   int bs = Hist::bin(value);
   int be = std::min(int(Hist::nbins() - 1), bs + n);
   bs = std::max(0, bs - n);
@@ -140,7 +155,7 @@ __host__ __device__ __forceinline__ void forEachInBins(Hist const &hist, V value
 
 // iteratate over bins containing all values in window wmin, wmax
 template <typename Hist, typename V, typename Func>
-__host__ __device__ __forceinline__ void forEachInWindow(Hist const &hist, V wmin, V wmax, Func const &func) {
+ALPAKA_FN_ACC __forceinline__ void forEachInWindow(Hist const &hist, V wmin, V wmax, Func const &func) {
   auto bs = Hist::bin(wmin);
   auto be = Hist::bin(wmax);
   assert(be >= bs);
@@ -191,7 +206,7 @@ public:
 
   static constexpr auto histOff(uint32_t nh) { return NBINS * nh; }
 
-  __host__ static size_t wsSize() {
+   static size_t wsSize() {
 #ifdef __CUDACC__
     uint32_t *v = nullptr;
     void *d_temp_storage = nullptr;
@@ -209,12 +224,12 @@ public:
     return (t >> shift) & mask;
   }
 
-  __host__ __device__ void zero() {
+  ALPAKA_FN_ACC void zero() {
     for (auto &i : off)
       i = 0;
   }
 
-  __host__ __device__ void add(CountersOnly const &co) {
+  ALPAKA_FN_ACC void add(CountersOnly const &co) {
     for (uint32_t i = 0; i < totbins(); ++i) {
 #ifdef __CUDA_ARCH__
       atomicAdd(off + i, co.off[i]);
@@ -224,7 +239,7 @@ public:
     }
   }
 
-  static __host__ __device__ __forceinline__ uint32_t atomicIncrement(Counter &x) {
+  static ALPAKA_FN_ACC __forceinline__ uint32_t atomicIncrement(Counter &x) {
 #ifdef __CUDA_ARCH__
     return atomicAdd(&x, 1);
 #else
@@ -232,7 +247,7 @@ public:
 #endif
   }
 
-  static __host__ __device__ __forceinline__ uint32_t atomicDecrement(Counter &x) {
+  static ALPAKA_FN_ACC __forceinline__ uint32_t atomicDecrement(Counter &x) {
 #ifdef __CUDA_ARCH__
     return atomicSub(&x, 1);
 #else
@@ -240,19 +255,19 @@ public:
 #endif
   }
 
-  __host__ __device__ __forceinline__ void countDirect(T b) {
+  ALPAKA_FN_ACC __forceinline__ void countDirect(T b) {
     assert(b < nbins());
     atomicIncrement(off[b]);
   }
 
-  __host__ __device__ __forceinline__ void fillDirect(T b, index_type j) {
+  ALPAKA_FN_ACC __forceinline__ void fillDirect(T b, index_type j) {
     assert(b < nbins());
     auto w = atomicDecrement(off[b]);
     assert(w > 0);
     bins[w - 1] = j;
   }
 
-  __device__ __host__ __forceinline__ int32_t bulkFill(AtomicPairCounter &apc, index_type const *v, uint32_t n) {
+  ALPAKA_FN_ACC __forceinline__ int32_t bulkFill(AtomicPairCounter &apc, index_type const *v, uint32_t n) {
     auto c = apc.add(n);
     if (c.m >= nbins())
       return -int32_t(c.m);
@@ -262,11 +277,12 @@ public:
     return c.m;
   }
 
-  __device__ __host__ __forceinline__ void bulkFinalize(AtomicPairCounter const &apc) {
+  ALPAKA_FN_ACC __forceinline__ void bulkFinalize(AtomicPairCounter const &apc) {
     off[apc.get().m] = apc.get().n;
   }
 
-  __device__ __host__ __forceinline__ void bulkFinalizeFill(AtomicPairCounter const &apc) {
+  template< typename T_Acc >
+  ALPAKA_FN_ACC __forceinline__ void bulkFinalizeFill(T_Acc const & acc, AtomicPairCounter const &apc) {
     auto m = apc.get().m;
     auto n = apc.get().n;
     auto first = m + blockDim.x * blockIdx.x + threadIdx.x;
@@ -275,13 +291,13 @@ public:
     }
   }
 
-  __host__ __device__ __forceinline__ void count(T t) {
+  ALPAKA_FN_ACC __forceinline__ void count(T t) {
     uint32_t b = bin(t);
     assert(b < nbins());
     atomicIncrement(off[b]);
   }
 
-  __host__ __device__ __forceinline__ void fill(T t, index_type j) {
+  ALPAKA_FN_ACC __forceinline__ void fill(T t, index_type j) {
     uint32_t b = bin(t);
     assert(b < nbins());
     auto w = atomicDecrement(off[b]);
@@ -289,7 +305,7 @@ public:
     bins[w - 1] = j;
   }
 
-  __host__ __device__ __forceinline__ void count(T t, uint32_t nh) {
+  ALPAKA_FN_ACC __forceinline__ void count(T t, uint32_t nh) {
     uint32_t b = bin(t);
     assert(b < nbins());
     b += histOff(nh);
@@ -297,7 +313,7 @@ public:
     atomicIncrement(off[b]);
   }
 
-  __host__ __device__ __forceinline__ void fill(T t, index_type j, uint32_t nh) {
+  ALPAKA_FN_ACC __forceinline__ void fill(T t, index_type j, uint32_t nh) {
     uint32_t b = bin(t);
     assert(b < nbins());
     b += histOff(nh);
@@ -307,7 +323,7 @@ public:
     bins[w - 1] = j;
   }
 
-  __device__ __host__ __forceinline__ void finalize(Counter *ws = nullptr) {
+ ALPAKA_FN_ACC  __forceinline__ void finalize(Counter *ws = nullptr) {
     assert(off[totbins() - 1] == 0);
     blockPrefixScan(off, totbins(), ws);
     assert(off[totbins() - 1] == off[totbins() - 2]);
diff --git a/HeterogeneousCore/CUDAUtilities/interface/cudaCheck.h b/HeterogeneousCore/CUDAUtilities/interface/cudaCheck.h
index e9fe7aba208..c49a5670abc 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/cudaCheck.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/cudaCheck.h
@@ -3,8 +3,9 @@
 
 #include <iostream>
 #include <sstream>
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
 #include <cuda.h>
-#include <cuda_runtime.h>
+//#include <cuda_runtime.h>
 
 namespace {
 
diff --git a/HeterogeneousCore/CUDAUtilities/interface/cudaCompat.h b/HeterogeneousCore/CUDAUtilities/interface/cudaCompat.h
index e5e5e18d08d..50e63800652 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/cudaCompat.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/cudaCompat.h
@@ -5,7 +5,7 @@
  * Everything you need to run cuda code in plain sequential c++ code
  */
 
-#ifndef __CUDACC__
+#if !defined __CUDACC__  &&  !defined CUPLA_KERNEL
 
 #include <algorithm>
 #include <cstdint>
diff --git a/HeterogeneousCore/CUDAUtilities/interface/host_noncached_unique_ptr.h b/HeterogeneousCore/CUDAUtilities/interface/host_noncached_unique_ptr.h
index c9f9aff89d9..83230e87936 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/host_noncached_unique_ptr.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/host_noncached_unique_ptr.h
@@ -14,7 +14,7 @@ namespace cudautils {
         class HostDeleter {
         public:
           void operator()(void *ptr) {
-            cuda::throw_if_error(cudaFreeHost(ptr));
+            throw_if_error(cudaFreeHost(ptr));
           }
         };
       }
@@ -42,7 +42,7 @@ namespace cudautils {
   make_host_noncached_unique(unsigned int flags = cudaHostAllocDefault) {
     static_assert(std::is_trivially_constructible<T>::value, "Allocating with non-trivial constructor on the pinned host memory is not supported");
     void *mem;
-    cuda::throw_if_error(cudaHostAlloc(&mem, sizeof(T), flags));
+    throw_if_error(cudaHostAlloc(&mem, sizeof(T), flags));
     return typename cudautils::host::noncached::impl::make_host_unique_selector<T>::non_array(reinterpret_cast<T *>(mem));
   }
 
@@ -52,7 +52,7 @@ namespace cudautils {
     using element_type = typename std::remove_extent<T>::type;
     static_assert(std::is_trivially_constructible<element_type>::value, "Allocating with non-trivial constructor on the pinned host memory is not supported");
     void *mem;
-    cuda::throw_if_error(cudaHostAlloc(&mem, n*sizeof(element_type), flags));
+    throw_if_error(cudaHostAlloc(&mem, n*sizeof(element_type), flags));
     return typename cudautils::host::noncached::impl::make_host_unique_selector<T>::unbounded_array(reinterpret_cast<element_type *>(mem));
   }
 
diff --git a/HeterogeneousCore/CUDAUtilities/interface/prefixScan.h b/HeterogeneousCore/CUDAUtilities/interface/prefixScan.h
index 20bea6ecb6f..e2da05fa219 100644
--- a/HeterogeneousCore/CUDAUtilities/interface/prefixScan.h
+++ b/HeterogeneousCore/CUDAUtilities/interface/prefixScan.h
@@ -7,19 +7,24 @@
 #include "HeterogeneousCore/CUDAUtilities/interface/cuda_assert.h"
 
 #ifdef __CUDA_ARCH__
-template <typename T>
-__device__ void __forceinline__ warpPrefixScan(T const* __restrict__ ci, T* __restrict__ co, uint32_t i, uint32_t mask) {
-  // ci and co may be the same
-  auto x = ci[i];
-  auto laneId = threadIdx.x & 0x1f;
+template<typename T>
+struct warpPrefixScan
+{
+    template< typename T_Acc >
+    ALPAKA_FN_ACC
+    void operator()( T_Acc const & acc, T const* __restrict__ ci, T* __restrict__ co, uint32_t i, uint32_t mask) const {
+    // ci and co may be the same
+    auto x = ci[i];
+    auto laneId = threadIdx.x & 0x1f;
 #pragma unroll
-  for (int offset = 1; offset < 32; offset <<= 1) {
-    auto y = __shfl_up_sync(mask, x, offset);
-    if (laneId >= offset)
-      x += y;
+    for (int offset = 1; offset < 32; offset <<= 1) {
+      auto y = __shfl_up_sync(mask, x, offset);
+      if (laneId >= offset)
+        x += y;
+    }
+    co[i] = x;
   }
-  co[i] = x;
-}
+};
 #endif
 
 //same as above may remove
@@ -126,44 +131,49 @@ __device__ __host__ void __forceinline__ blockPrefixScan(T* c,
 }
 
 // limited to 1024*1024 elements....
-template <typename T>
-__global__ void multiBlockPrefixScan(T const* __restrict__ ci, T* __restrict__ co, int32_t size, int32_t* pc) {
-  __shared__ T ws[32];
-  // first each block does a scan of size 1024; (better be enough blocks....)
-  assert(1024 * gridDim.x >= size);
-  int off = 1024 * blockIdx.x;
-  if (size - off > 0)
-    blockPrefixScan(ci + off, co + off, std::min(1024, size - off), ws);
-
-  // count blocks that finished
-  __shared__ bool isLastBlockDone;
-  if (0 == threadIdx.x) {
-    auto value = atomicAdd(pc, 1);  // block counter
-    isLastBlockDone = (value == (gridDim.x - 1));
-  }
-
-  __syncthreads();
-
-  if (!isLastBlockDone)
-    return;
-
-  // good each block has done its work and now we are left in last block
-
-  // let's get the partial sums from each block
-  __shared__ T psum[1024];
-  for (int i = threadIdx.x; i < gridDim.x; i += blockDim.x) {
-    auto j = 1024 * i + 1023;
-    psum[i] = (j < size) ? co[j] : T(0);
+template<typename T>
+struct multiBlockPrefixScan
+{
+  template< typename T_Acc >
+  ALPAKA_FN_ACC
+  void operator()( T_Acc const & acc, T const* __restrict__ ci, T* __restrict__ co, int32_t size, int32_t* pc) const {
+    __shared__ T ws[32];
+    // first each block does a scan of size 1024; (better be enough blocks....)
+    assert(1024 * gridDim.x >= size);
+    int off = 1024 * blockIdx.x;
+    if (size - off > 0)
+      blockPrefixScan(ci + off, co + off, std::min(1024, size - off), ws);
+
+    // count blocks that finished
+    __shared__ bool isLastBlockDone;
+    if (0 == threadIdx.x) {
+      auto value = atomicAdd(pc, 1);  // block counter
+      isLastBlockDone = (value == (gridDim.x - 1));
+    }
+
+    __syncthreads();
+
+    if (!isLastBlockDone)
+      return;
+
+    // good each block has done its work and now we are left in last block
+
+    // let's get the partial sums from each block
+    __shared__ T psum[1024];
+    for (int i = threadIdx.x; i < gridDim.x; i += blockDim.x) {
+      auto j = 1024 * i + 1023;
+      psum[i] = (j < size) ? co[j] : T(0);
+    }
+    __syncthreads();
+    blockPrefixScan(psum, psum, gridDim.x, ws);
+
+    // now it would have been handy to have the other blocks around...
+    int first = threadIdx.x;                                 // + blockDim.x * blockIdx.x
+    for (int i = first + 1024; i < size; i += blockDim.x) {  //  *gridDim.x) {
+      auto k = i / 1024;                                     // block
+      co[i] += psum[k - 1];
+    }
   }
-  __syncthreads();
-  blockPrefixScan(psum, psum, gridDim.x, ws);
-
-  // now it would have been handy to have the other blocks around...
-  int first = threadIdx.x;                                 // + blockDim.x * blockIdx.x
-  for (int i = first + 1024; i < size; i += blockDim.x) {  //  *gridDim.x) {
-    auto k = i / 1024;                                     // block
-    co[i] += psum[k - 1];
-  }
-}
+};
 
 #endif  // HeterogeneousCore_CUDAUtilities_interface_prefixScan_h
diff --git a/RecoLocalTracker/SiPixelClusterizer/interface/SiPixelFedCablingMapGPU.h b/RecoLocalTracker/SiPixelClusterizer/interface/SiPixelFedCablingMapGPU.h
index aeb7ade62af..ab509111b83 100644
--- a/RecoLocalTracker/SiPixelClusterizer/interface/SiPixelFedCablingMapGPU.h
+++ b/RecoLocalTracker/SiPixelClusterizer/interface/SiPixelFedCablingMapGPU.h
@@ -13,13 +13,13 @@ namespace pixelgpudetails {
 
 // TODO: since this has more information than just cabling map, maybe we should invent a better name?
 struct SiPixelFedCablingMapGPU {
-  unsigned int fed[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned int link[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned int roc[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned int RawId[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned int rocInDet[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned int moduleId[pixelgpudetails::MAX_SIZE] alignas(128);
-  unsigned char badRocs[pixelgpudetails::MAX_SIZE] alignas(128);
+  alignas(128) unsigned int fed[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned int link[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned int roc[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned int RawId[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned int rocInDet[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned int moduleId[pixelgpudetails::MAX_SIZE];
+  alignas(128) unsigned char badRocs[pixelgpudetails::MAX_SIZE];
   unsigned int size = 0;
 };
 
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/BuildFile.xml b/RecoLocalTracker/SiPixelClusterizer/plugins/BuildFile.xml
index 40a489f7633..c482324e21d 100644
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/BuildFile.xml
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/BuildFile.xml
@@ -13,8 +13,9 @@
 <use   name="HeterogeneousCore/Product"/>
 <use   name="HeterogeneousCore/CUDACore"/>
 <use   name="cuda"/>
-<use   name="cuda-api-wrappers"/>
 <use   name="cub"/>
-<library   file="*.cc *.cu" name="RecoLocalTrackerSiPixelClusterizerPlugins">
+<use name="boost"/>
+<flags CXXFLAGS="-g -I/data/user/dmarcato/cupla/include/ -I/data/user/dmarcato/alpaka/include"/>
+<library   file="*.cc" name="RecoLocalTrackerSiPixelClusterizerPlugins">
   <flags   EDM_PLUGIN="1"/>
-</library>
+</library>
\ No newline at end of file
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu b/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu
deleted file mode 100644
index 8a5119d6848..00000000000
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu
+++ /dev/null
@@ -1,664 +0,0 @@
-/* Sushil Dubey, Shashi Dugad, TIFR, July 2017
- *
- * File Name: RawToClusterGPU.cu
- * Description: It converts Raw data into Digi Format on GPU
- * Finaly the Output of RawToDigi data is given to pixelClusterizer
- *
-**/
-
-// C++ includes
-#include <cassert>
-#include <chrono>
-#include <cstdio>
-#include <cstdlib>
-#include <fstream>
-#include <iomanip>
-#include <iostream>
-#include <string>
-
-// CUDA includes
-#include <cuda.h>
-#include <cuda_runtime.h>
-#include <thrust/device_vector.h>
-#include <thrust/execution_policy.h>
-#include <thrust/host_vector.h>
-#include <thrust/sort.h>
-#include <thrust/unique.h>
-
-// cub includes
-#include <cub/cub.cuh>
-
-// CMSSW includes
-#include "CUDADataFormats/SiPixelCluster/interface/gpuClusteringConstants.h"
-#include "FWCore/ServiceRegistry/interface/Service.h"
-#include "HeterogeneousCore/CUDAServices/interface/CUDAService.h"
-#include "HeterogeneousCore/CUDAUtilities/interface/cudaCheck.h"
-#include "RecoLocalTracker/SiPixelClusterizer/interface/SiPixelFedCablingMapGPU.h"
-#include "RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h"
-#include "RecoLocalTracker/SiPixelClusterizer/plugins/gpuClusterChargeCut.h"
-#include "RecoLocalTracker/SiPixelClusterizer/plugins/gpuClustering.h"
-
-// local includes
-#include "SiPixelRawToClusterGPUKernel.h"
-
-namespace pixelgpudetails {
-
-  // number of words for all the FEDs
-  constexpr uint32_t MAX_FED_WORDS = pixelgpudetails::MAX_FED * pixelgpudetails::MAX_WORD;
-
-  SiPixelRawToClusterGPUKernel::WordFedAppender::WordFedAppender() {
-    word_ = cudautils::make_host_noncached_unique<unsigned int[]>(MAX_FED_WORDS, cudaHostAllocWriteCombined);
-    fedId_ = cudautils::make_host_noncached_unique<unsigned char[]>(MAX_FED_WORDS, cudaHostAllocWriteCombined);
-  }
-
-  void SiPixelRawToClusterGPUKernel::WordFedAppender::initializeWordFed(int fedId,
-                                                                        unsigned int wordCounterGPU,
-                                                                        const cms_uint32_t *src,
-                                                                        unsigned int length) {
-    std::memcpy(word_.get() + wordCounterGPU, src, sizeof(cms_uint32_t) * length);
-    std::memset(fedId_.get() + wordCounterGPU / 2, fedId - 1200, length / 2);
-  }
-
-  ////////////////////
-
-  __device__ uint32_t getLink(uint32_t ww) {
-    return ((ww >> pixelgpudetails::LINK_shift) & pixelgpudetails::LINK_mask);
-  }
-
-  __device__ uint32_t getRoc(uint32_t ww) { return ((ww >> pixelgpudetails::ROC_shift) & pixelgpudetails::ROC_mask); }
-
-  __device__ uint32_t getADC(uint32_t ww) { return ((ww >> pixelgpudetails::ADC_shift) & pixelgpudetails::ADC_mask); }
-
-  __device__ bool isBarrel(uint32_t rawId) { return (1 == ((rawId >> 25) & 0x7)); }
-
-  __device__ pixelgpudetails::DetIdGPU getRawId(const SiPixelFedCablingMapGPU *cablingMap,
-                                                uint8_t fed,
-                                                uint32_t link,
-                                                uint32_t roc) {
-    uint32_t index = fed * MAX_LINK * MAX_ROC + (link - 1) * MAX_ROC + roc;
-    pixelgpudetails::DetIdGPU detId = {
-        cablingMap->RawId[index], cablingMap->rocInDet[index], cablingMap->moduleId[index]};
-    return detId;
-  }
-
-  //reference http://cmsdoxygen.web.cern.ch/cmsdoxygen/CMSSW_9_2_0/doc/html/dd/d31/FrameConversion_8cc_source.html
-  //http://cmslxr.fnal.gov/source/CondFormats/SiPixelObjects/src/PixelROC.cc?v=CMSSW_9_2_0#0071
-  // Convert local pixel to pixelgpudetails::global pixel
-  __device__ pixelgpudetails::Pixel frameConversion(
-      bool bpix, int side, uint32_t layer, uint32_t rocIdInDetUnit, pixelgpudetails::Pixel local) {
-    int slopeRow = 0, slopeCol = 0;
-    int rowOffset = 0, colOffset = 0;
-
-    if (bpix) {
-      if (side == -1 && layer != 1) {  // -Z side: 4 non-flipped modules oriented like 'dddd', except Layer 1
-        if (rocIdInDetUnit < 8) {
-          slopeRow = 1;
-          slopeCol = -1;
-          rowOffset = 0;
-          colOffset = (8 - rocIdInDetUnit) * pixelgpudetails::numColsInRoc - 1;
-        } else {
-          slopeRow = -1;
-          slopeCol = 1;
-          rowOffset = 2 * pixelgpudetails::numRowsInRoc - 1;
-          colOffset = (rocIdInDetUnit - 8) * pixelgpudetails::numColsInRoc;
-        }       // if roc
-      } else {  // +Z side: 4 non-flipped modules oriented like 'pppp', but all 8 in layer1
-        if (rocIdInDetUnit < 8) {
-          slopeRow = -1;
-          slopeCol = 1;
-          rowOffset = 2 * pixelgpudetails::numRowsInRoc - 1;
-          colOffset = rocIdInDetUnit * pixelgpudetails::numColsInRoc;
-        } else {
-          slopeRow = 1;
-          slopeCol = -1;
-          rowOffset = 0;
-          colOffset = (16 - rocIdInDetUnit) * pixelgpudetails::numColsInRoc - 1;
-        }
-      }
-
-    } else {             // fpix
-      if (side == -1) {  // pannel 1
-        if (rocIdInDetUnit < 8) {
-          slopeRow = 1;
-          slopeCol = -1;
-          rowOffset = 0;
-          colOffset = (8 - rocIdInDetUnit) * pixelgpudetails::numColsInRoc - 1;
-        } else {
-          slopeRow = -1;
-          slopeCol = 1;
-          rowOffset = 2 * pixelgpudetails::numRowsInRoc - 1;
-          colOffset = (rocIdInDetUnit - 8) * pixelgpudetails::numColsInRoc;
-        }
-      } else {  // pannel 2
-        if (rocIdInDetUnit < 8) {
-          slopeRow = 1;
-          slopeCol = -1;
-          rowOffset = 0;
-          colOffset = (8 - rocIdInDetUnit) * pixelgpudetails::numColsInRoc - 1;
-        } else {
-          slopeRow = -1;
-          slopeCol = 1;
-          rowOffset = 2 * pixelgpudetails::numRowsInRoc - 1;
-          colOffset = (rocIdInDetUnit - 8) * pixelgpudetails::numColsInRoc;
-        }
-
-      }  // side
-    }
-
-    uint32_t gRow = rowOffset + slopeRow * local.row;
-    uint32_t gCol = colOffset + slopeCol * local.col;
-    //printf("Inside frameConversion row: %u, column: %u\n", gRow, gCol);
-    pixelgpudetails::Pixel global = {gRow, gCol};
-    return global;
-  }
-
-  __device__ uint8_t conversionError(uint8_t fedId, uint8_t status, bool debug = false) {
-    uint8_t errorType = 0;
-
-    // debug = true;
-
-    switch (status) {
-      case (1): {
-        if (debug)
-          printf("Error in Fed: %i, invalid channel Id (errorType = 35\n)", fedId);
-        errorType = 35;
-        break;
-      }
-      case (2): {
-        if (debug)
-          printf("Error in Fed: %i, invalid ROC Id (errorType = 36)\n", fedId);
-        errorType = 36;
-        break;
-      }
-      case (3): {
-        if (debug)
-          printf("Error in Fed: %i, invalid dcol/pixel value (errorType = 37)\n", fedId);
-        errorType = 37;
-        break;
-      }
-      case (4): {
-        if (debug)
-          printf("Error in Fed: %i, dcol/pixel read out of order (errorType = 38)\n", fedId);
-        errorType = 38;
-        break;
-      }
-      default:
-        if (debug)
-          printf("Cabling check returned unexpected result, status = %i\n", status);
-    };
-
-    return errorType;
-  }
-
-  __device__ bool rocRowColIsValid(uint32_t rocRow, uint32_t rocCol) {
-    uint32_t numRowsInRoc = 80;
-    uint32_t numColsInRoc = 52;
-
-    /// row and collumn in ROC representation
-    return ((rocRow < numRowsInRoc) & (rocCol < numColsInRoc));
-  }
-
-  __device__ bool dcolIsValid(uint32_t dcol, uint32_t pxid) { return ((dcol < 26) & (2 <= pxid) & (pxid < 162)); }
-
-  __device__ uint8_t checkROC(
-      uint32_t errorWord, uint8_t fedId, uint32_t link, const SiPixelFedCablingMapGPU *cablingMap, bool debug = false) {
-    uint8_t errorType = (errorWord >> pixelgpudetails::ROC_shift) & pixelgpudetails::ERROR_mask;
-    if (errorType < 25)
-      return 0;
-    bool errorFound = false;
-
-    switch (errorType) {
-      case (25): {
-        errorFound = true;
-        uint32_t index = fedId * MAX_LINK * MAX_ROC + (link - 1) * MAX_ROC + 1;
-        if (index > 1 && index <= cablingMap->size) {
-          if (!(link == cablingMap->link[index] && 1 == cablingMap->roc[index]))
-            errorFound = false;
-        }
-        if (debug and errorFound)
-          printf("Invalid ROC = 25 found (errorType = 25)\n");
-        break;
-      }
-      case (26): {
-        if (debug)
-          printf("Gap word found (errorType = 26)\n");
-        errorFound = true;
-        break;
-      }
-      case (27): {
-        if (debug)
-          printf("Dummy word found (errorType = 27)\n");
-        errorFound = true;
-        break;
-      }
-      case (28): {
-        if (debug)
-          printf("Error fifo nearly full (errorType = 28)\n");
-        errorFound = true;
-        break;
-      }
-      case (29): {
-        if (debug)
-          printf("Timeout on a channel (errorType = 29)\n");
-        if ((errorWord >> pixelgpudetails::OMIT_ERR_shift) & pixelgpudetails::OMIT_ERR_mask) {
-          if (debug)
-            printf("...first errorType=29 error, this gets masked out\n");
-        }
-        errorFound = true;
-        break;
-      }
-      case (30): {
-        if (debug)
-          printf("TBM error trailer (errorType = 30)\n");
-        int StateMatch_bits = 4;
-        int StateMatch_shift = 8;
-        uint32_t StateMatch_mask = ~(~uint32_t(0) << StateMatch_bits);
-        int StateMatch = (errorWord >> StateMatch_shift) & StateMatch_mask;
-        if (StateMatch != 1 && StateMatch != 8) {
-          if (debug)
-            printf("FED error 30 with unexpected State Bits (errorType = 30)\n");
-        }
-        if (StateMatch == 1)
-          errorType = 40;  // 1=Overflow -> 40, 8=number of ROCs -> 30
-        errorFound = true;
-        break;
-      }
-      case (31): {
-        if (debug)
-          printf("Event number error (errorType = 31)\n");
-        errorFound = true;
-        break;
-      }
-      default:
-        errorFound = false;
-    };
-
-    return errorFound ? errorType : 0;
-  }
-
-  __device__ uint32_t getErrRawID(uint8_t fedId,
-                                  uint32_t errWord,
-                                  uint32_t errorType,
-                                  const SiPixelFedCablingMapGPU *cablingMap,
-                                  bool debug = false) {
-    uint32_t rID = 0xffffffff;
-
-    switch (errorType) {
-      case 25:
-      case 30:
-      case 31:
-      case 36:
-      case 40: {
-        //set dummy values for cabling just to get detId from link
-        //cabling.dcol = 0;
-        //cabling.pxid = 2;
-        uint32_t roc = 1;
-        uint32_t link = (errWord >> pixelgpudetails::LINK_shift) & pixelgpudetails::LINK_mask;
-        uint32_t rID_temp = getRawId(cablingMap, fedId, link, roc).RawId;
-        if (rID_temp != 9999)
-          rID = rID_temp;
-        break;
-      }
-      case 29: {
-        int chanNmbr = 0;
-        const int DB0_shift = 0;
-        const int DB1_shift = DB0_shift + 1;
-        const int DB2_shift = DB1_shift + 1;
-        const int DB3_shift = DB2_shift + 1;
-        const int DB4_shift = DB3_shift + 1;
-        const uint32_t DataBit_mask = ~(~uint32_t(0) << 1);
-
-        int CH1 = (errWord >> DB0_shift) & DataBit_mask;
-        int CH2 = (errWord >> DB1_shift) & DataBit_mask;
-        int CH3 = (errWord >> DB2_shift) & DataBit_mask;
-        int CH4 = (errWord >> DB3_shift) & DataBit_mask;
-        int CH5 = (errWord >> DB4_shift) & DataBit_mask;
-        int BLOCK_bits = 3;
-        int BLOCK_shift = 8;
-        uint32_t BLOCK_mask = ~(~uint32_t(0) << BLOCK_bits);
-        int BLOCK = (errWord >> BLOCK_shift) & BLOCK_mask;
-        int localCH = 1 * CH1 + 2 * CH2 + 3 * CH3 + 4 * CH4 + 5 * CH5;
-        if (BLOCK % 2 == 0)
-          chanNmbr = (BLOCK / 2) * 9 + localCH;
-        else
-          chanNmbr = ((BLOCK - 1) / 2) * 9 + 4 + localCH;
-        if ((chanNmbr < 1) || (chanNmbr > 36))
-          break;  // signifies unexpected result
-
-        // set dummy values for cabling just to get detId from link if in Barrel
-        //cabling.dcol = 0;
-        //cabling.pxid = 2;
-        uint32_t roc = 1;
-        uint32_t link = chanNmbr;
-        uint32_t rID_temp = getRawId(cablingMap, fedId, link, roc).RawId;
-        if (rID_temp != 9999)
-          rID = rID_temp;
-        break;
-      }
-      case 37:
-      case 38: {
-        //cabling.dcol = 0;
-        //cabling.pxid = 2;
-        uint32_t roc = (errWord >> pixelgpudetails::ROC_shift) & pixelgpudetails::ROC_mask;
-        uint32_t link = (errWord >> pixelgpudetails::LINK_shift) & pixelgpudetails::LINK_mask;
-        uint32_t rID_temp = getRawId(cablingMap, fedId, link, roc).RawId;
-        if (rID_temp != 9999)
-          rID = rID_temp;
-        break;
-      }
-      default:
-        break;
-    };
-
-    return rID;
-  }
-
-  // Kernel to perform Raw to Digi conversion
-  __global__ void RawToDigi_kernel(const SiPixelFedCablingMapGPU *cablingMap,
-                                   const unsigned char *modToUnp,
-                                   const uint32_t wordCounter,
-                                   const uint32_t *word,
-                                   const uint8_t *fedIds,
-                                   uint16_t *xx,
-                                   uint16_t *yy,
-                                   uint16_t *adc,
-                                   uint32_t *pdigi,
-                                   uint32_t *rawIdArr,
-                                   uint16_t *moduleId,
-                                   GPU::SimpleVector<PixelErrorCompact> *err,
-                                   bool useQualityInfo,
-                                   bool includeErrors,
-                                   bool debug) {
-    //if (threadIdx.x==0) printf("Event: %u blockIdx.x: %u start: %u end: %u\n", eventno, blockIdx.x, begin, end);
-
-    int32_t first = threadIdx.x + blockIdx.x * blockDim.x;
-    for (int32_t iloop = first, nend = wordCounter; iloop < nend; iloop += blockDim.x * gridDim.x) {
-      auto gIndex = iloop;
-      xx[gIndex] = 0;
-      yy[gIndex] = 0;
-      adc[gIndex] = 0;
-      bool skipROC = false;
-
-      uint8_t fedId = fedIds[gIndex / 2];  // +1200;
-
-      // initialize (too many coninue below)
-      pdigi[gIndex] = 0;
-      rawIdArr[gIndex] = 0;
-      moduleId[gIndex] = 9999;
-
-      uint32_t ww = word[gIndex];  // Array containing 32 bit raw data
-      if (ww == 0) {
-        // 0 is an indicator of a noise/dead channel, skip these pixels during clusterization
-        continue;
-      }
-
-      uint32_t link = getLink(ww);  // Extract link
-      uint32_t roc = getRoc(ww);    // Extract Roc in link
-      pixelgpudetails::DetIdGPU detId = getRawId(cablingMap, fedId, link, roc);
-
-      uint8_t errorType = checkROC(ww, fedId, link, cablingMap, debug);
-      skipROC = (roc < pixelgpudetails::maxROCIndex) ? false : (errorType != 0);
-      if (includeErrors and skipROC) {
-        uint32_t rID = getErrRawID(fedId, ww, errorType, cablingMap, debug);
-        err->push_back(PixelErrorCompact{rID, ww, errorType, fedId});
-        continue;
-      }
-
-      uint32_t rawId = detId.RawId;
-      uint32_t rocIdInDetUnit = detId.rocInDet;
-      bool barrel = isBarrel(rawId);
-
-      uint32_t index = fedId * MAX_LINK * MAX_ROC + (link - 1) * MAX_ROC + roc;
-      if (useQualityInfo) {
-        skipROC = cablingMap->badRocs[index];
-        if (skipROC)
-          continue;
-      }
-      skipROC = modToUnp[index];
-      if (skipROC)
-        continue;
-
-      uint32_t layer = 0;                   //, ladder =0;
-      int side = 0, panel = 0, module = 0;  //disk = 0, blade = 0
-
-      if (barrel) {
-        layer = (rawId >> pixelgpudetails::layerStartBit) & pixelgpudetails::layerMask;
-        module = (rawId >> pixelgpudetails::moduleStartBit) & pixelgpudetails::moduleMask;
-        side = (module < 5) ? -1 : 1;
-      } else {
-        // endcap ids
-        layer = 0;
-        panel = (rawId >> pixelgpudetails::panelStartBit) & pixelgpudetails::panelMask;
-        //disk  = (rawId >> diskStartBit_) & diskMask_;
-        side = (panel == 1) ? -1 : 1;
-        //blade = (rawId >> bladeStartBit_) & bladeMask_;
-      }
-
-      // ***special case of layer to 1 be handled here
-      pixelgpudetails::Pixel localPix;
-      if (layer == 1) {
-        uint32_t col = (ww >> pixelgpudetails::COL_shift) & pixelgpudetails::COL_mask;
-        uint32_t row = (ww >> pixelgpudetails::ROW_shift) & pixelgpudetails::ROW_mask;
-        localPix.row = row;
-        localPix.col = col;
-        if (includeErrors) {
-          if (not rocRowColIsValid(row, col)) {
-            uint8_t error = conversionError(fedId, 3, debug);  //use the device function and fill the arrays
-            err->push_back(PixelErrorCompact{rawId, ww, error, fedId});
-            if (debug)
-              printf("BPIX1  Error status: %i\n", error);
-            continue;
-          }
-        }
-      } else {
-        // ***conversion rules for dcol and pxid
-        uint32_t dcol = (ww >> pixelgpudetails::DCOL_shift) & pixelgpudetails::DCOL_mask;
-        uint32_t pxid = (ww >> pixelgpudetails::PXID_shift) & pixelgpudetails::PXID_mask;
-        uint32_t row = pixelgpudetails::numRowsInRoc - pxid / 2;
-        uint32_t col = dcol * 2 + pxid % 2;
-        localPix.row = row;
-        localPix.col = col;
-        if (includeErrors and not dcolIsValid(dcol, pxid)) {
-          uint8_t error = conversionError(fedId, 3, debug);
-          err->push_back(PixelErrorCompact{rawId, ww, error, fedId});
-          if (debug)
-            printf("Error status: %i %d %d %d %d\n", error, dcol, pxid, fedId, roc);
-          continue;
-        }
-      }
-
-      pixelgpudetails::Pixel globalPix = frameConversion(barrel, side, layer, rocIdInDetUnit, localPix);
-      xx[gIndex] = globalPix.row;  // origin shifting by 1 0-159
-      yy[gIndex] = globalPix.col;  // origin shifting by 1 0-415
-      adc[gIndex] = getADC(ww);
-      pdigi[gIndex] = pixelgpudetails::pack(globalPix.row, globalPix.col, adc[gIndex]);
-      moduleId[gIndex] = detId.moduleId;
-      rawIdArr[gIndex] = rawId;
-    }  // end of loop (gIndex < end)
-
-  }  // end of Raw to Digi kernel
-
-  __global__ void fillHitsModuleStart(uint32_t const *__restrict__ cluStart, uint32_t *__restrict__ moduleStart) {
-    assert(gpuClustering::MaxNumModules < 2048);  // easy to extend at least till 32*1024
-    assert(1 == gridDim.x);
-    assert(0 == blockIdx.x);
-
-    int first = threadIdx.x;
-
-    // limit to MaxHitsInModule;
-    for (int i = first, iend = gpuClustering::MaxNumModules; i < iend; i += blockDim.x) {
-      moduleStart[i + 1] = std::min(gpuClustering::maxHitsInModule(), cluStart[i]);
-    }
-
-    __shared__ uint32_t ws[32];
-    blockPrefixScan(moduleStart + 1, moduleStart + 1, 1024, ws);
-    blockPrefixScan(moduleStart + 1025, moduleStart + 1025, gpuClustering::MaxNumModules - 1024, ws);
-
-    for (int i = first + 1025, iend = gpuClustering::MaxNumModules + 1; i < iend; i += blockDim.x) {
-      moduleStart[i] += moduleStart[1024];
-    }
-    __syncthreads();
-
-#ifdef GPU_DEBUG
-    assert(0 == moduleStart[0]);
-    auto c0 = std::min(gpuClustering::maxHitsInModule(), cluStart[0]);
-    assert(c0 == moduleStart[1]);
-    assert(moduleStart[1024] >= moduleStart[1023]);
-    assert(moduleStart[1025] >= moduleStart[1024]);
-    assert(moduleStart[gpuClustering::MaxNumModules] >= moduleStart[1025]);
-
-    for (int i = first, iend = gpuClustering::MaxNumModules + 1; i < iend; i += blockDim.x) {
-      if (0 != i)
-        assert(moduleStart[i] >= moduleStart[i - i]);
-      // [BPX1, BPX2, BPX3, BPX4,  FP1,  FP2,  FP3,  FN1,  FN2,  FN3, LAST_VALID]
-      // [   0,   96,  320,  672, 1184, 1296, 1408, 1520, 1632, 1744,       1856]
-      if (i == 96 || i == 1184 || i == 1744 || i == gpuClustering::MaxNumModules)
-        printf("moduleStart %d %d\n", i, moduleStart[i]);
-    }
-#endif
-
-    // avoid overflow
-    constexpr auto MAX_HITS = gpuClustering::MaxNumClusters;
-    for (int i = first, iend = gpuClustering::MaxNumModules + 1; i < iend; i += blockDim.x) {
-      if (moduleStart[i] > MAX_HITS)
-        moduleStart[i] = MAX_HITS;
-    }
-  }
-
-  // Interface to outside
-  void SiPixelRawToClusterGPUKernel::makeClustersAsync(const SiPixelFedCablingMapGPU *cablingMap,
-                                                       const unsigned char *modToUnp,
-                                                       const SiPixelGainForHLTonGPU *gains,
-                                                       const WordFedAppender &wordFed,
-                                                       PixelFormatterErrors &&errors,
-                                                       const uint32_t wordCounter,
-                                                       const uint32_t fedCounter,
-                                                       bool useQualityInfo,
-                                                       bool includeErrors,
-                                                       bool debug,
-                                                       cuda::stream_t<> &stream) {
-    nDigis = wordCounter;
-
-    digis_d = SiPixelDigisCUDA(pixelgpudetails::MAX_FED_WORDS, stream);
-    if (includeErrors) {
-      digiErrors_d = SiPixelDigiErrorsCUDA(pixelgpudetails::MAX_FED_WORDS, std::move(errors), stream);
-    }
-    clusters_d = SiPixelClustersCUDA(gpuClustering::MaxNumModules, stream);
-
-    edm::Service<CUDAService> cs;
-    nModules_Clusters_h = cs->make_host_unique<uint32_t[]>(2, stream);
-
-    if (wordCounter)  // protect in case of empty event....
-    {
-      const int threadsPerBlock = 512;
-      const int blocks = (wordCounter + threadsPerBlock - 1) / threadsPerBlock;  // fill it all
-
-      assert(0 == wordCounter % 2);
-      // wordCounter is the total no of words in each event to be trasfered on device
-      auto word_d = cs->make_device_unique<uint32_t[]>(wordCounter, stream);
-      auto fedId_d = cs->make_device_unique<uint8_t[]>(wordCounter, stream);
-
-      cudaCheck(cudaMemcpyAsync(
-          word_d.get(), wordFed.word(), wordCounter * sizeof(uint32_t), cudaMemcpyDefault, stream.id()));
-      cudaCheck(cudaMemcpyAsync(
-          fedId_d.get(), wordFed.fedId(), wordCounter * sizeof(uint8_t) / 2, cudaMemcpyDefault, stream.id()));
-
-      // Launch rawToDigi kernel
-      RawToDigi_kernel<<<blocks, threadsPerBlock, 0, stream.id()>>>(
-          cablingMap,
-          modToUnp,
-          wordCounter,
-          word_d.get(),
-          fedId_d.get(),
-          digis_d.xx(),
-          digis_d.yy(),
-          digis_d.adc(),
-          digis_d.pdigi(),
-          digis_d.rawIdArr(),
-          digis_d.moduleInd(),
-          digiErrors_d.error(),  // returns nullptr if default-constructed
-          useQualityInfo,
-          includeErrors,
-          debug);
-      cudaCheck(cudaGetLastError());
-
-      if (includeErrors) {
-        digiErrors_d.copyErrorToHostAsync(stream);
-      }
-    }
-    // End of Raw2Digi and passing data for clustering
-
-    {
-      // clusterizer ...
-      using namespace gpuClustering;
-      int threadsPerBlock = 256;
-      int blocks =
-          (std::max(int(wordCounter), int(gpuClustering::MaxNumModules)) + threadsPerBlock - 1) / threadsPerBlock;
-
-      gpuCalibPixel::calibDigis<<<blocks, threadsPerBlock, 0, stream.id()>>>(digis_d.moduleInd(),
-                                                                             digis_d.c_xx(),
-                                                                             digis_d.c_yy(),
-                                                                             digis_d.adc(),
-                                                                             gains,
-                                                                             wordCounter,
-                                                                             clusters_d.moduleStart(),
-                                                                             clusters_d.clusInModule(),
-                                                                             clusters_d.clusModuleStart());
-      cudaCheck(cudaGetLastError());
-
-#ifdef GPU_DEBUG
-      std::cout << "CUDA countModules kernel launch with " << blocks << " blocks of " << threadsPerBlock
-                << " threads\n";
-#endif
-
-      countModules<<<blocks, threadsPerBlock, 0, stream.id()>>>(
-          digis_d.c_moduleInd(), clusters_d.moduleStart(), digis_d.clus(), wordCounter);
-      cudaCheck(cudaGetLastError());
-
-      // read the number of modules into a data member, used by getProduct())
-      cudaCheck(cudaMemcpyAsync(
-          &(nModules_Clusters_h[0]), clusters_d.moduleStart(), sizeof(uint32_t), cudaMemcpyDefault, stream.id()));
-
-      threadsPerBlock = 256;
-      blocks = MaxNumModules;
-#ifdef GPU_DEBUG
-      std::cout << "CUDA findClus kernel launch with " << blocks << " blocks of " << threadsPerBlock << " threads\n";
-#endif
-      findClus<<<blocks, threadsPerBlock, 0, stream.id()>>>(digis_d.c_moduleInd(),
-                                                            digis_d.c_xx(),
-                                                            digis_d.c_yy(),
-                                                            clusters_d.c_moduleStart(),
-                                                            clusters_d.clusInModule(),
-                                                            clusters_d.moduleId(),
-                                                            digis_d.clus(),
-                                                            wordCounter);
-      cudaCheck(cudaGetLastError());
-
-      // apply charge cut
-      clusterChargeCut<<<blocks, threadsPerBlock, 0, stream.id()>>>(digis_d.moduleInd(),
-                                                                    digis_d.c_adc(),
-                                                                    clusters_d.c_moduleStart(),
-                                                                    clusters_d.clusInModule(),
-                                                                    clusters_d.c_moduleId(),
-                                                                    digis_d.clus(),
-                                                                    wordCounter);
-      cudaCheck(cudaGetLastError());
-
-      // count the module start indices already here (instead of
-      // rechits) so that the number of clusters/hits can be made
-      // available in the rechit producer without additional points of
-      // synchronization/ExternalWork
-
-      // MUST be ONE block
-      fillHitsModuleStart<<<1, 1024, 0, stream.id()>>>(clusters_d.c_clusInModule(), clusters_d.clusModuleStart());
-
-      // last element holds the number of all clusters
-      cudaCheck(cudaMemcpyAsync(&(nModules_Clusters_h[1]),
-                                clusters_d.clusModuleStart() + gpuClustering::MaxNumModules,
-                                sizeof(uint32_t),
-                                cudaMemcpyDefault,
-                                stream.id()));
-
-    }  // end clusterizer scope
-  }
-}  // namespace pixelgpudetails
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu b/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu
new file mode 120000
index 00000000000..fa5d5e76e23
--- /dev/null
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu
@@ -0,0 +1 @@
+SiPixelRawToClusterGPUKernel.cc
\ No newline at end of file
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.h b/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.h
index 0d2b6a8c7fc..6d63e948d40 100644
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.h
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.h
@@ -2,8 +2,10 @@
 #define RecoLocalTracker_SiPixelClusterizer_plugins_SiPixelRawToClusterGPUKernel_h
 
 #include <algorithm>
-#include <cuda_runtime.h>
-#include "cuda/api_wrappers.h"
+
+// Cupla includes
+//#define CUPLA_STREAM_ASYNC_ENABLED 1
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
 
 #include "CUDADataFormats/SiPixelDigi/interface/SiPixelDigisCUDA.h"
 #include "CUDADataFormats/SiPixelDigi/interface/SiPixelDigiErrorsCUDA.h"
@@ -90,7 +92,7 @@ namespace pixelgpudetails {
     using PackedDigiType = uint32_t;
 
     // Constructor: pre-computes masks and shifts from field widths
-    __host__ __device__
+    ALPAKA_FN_ACC
     inline
     constexpr Packing(unsigned int row_w, unsigned int column_w,
                       unsigned int time_w, unsigned int adc_w) :
@@ -131,14 +133,14 @@ namespace pixelgpudetails {
     uint32_t  max_adc;
   };
 
-  __host__ __device__
+  ALPAKA_FN_ACC
   inline
   constexpr Packing packing() {
     return Packing(11, 11, 0, 10);
   }
 
 
-  __host__ __device__
+  ALPAKA_FN_ACC
   inline
   uint32_t pack(uint32_t row, uint32_t col, uint32_t adc) {
     constexpr Packing thePacking = packing();
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h
index 41e028b3c45..63b85a0699a 100644
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h
@@ -7,7 +7,8 @@
 #include "CondFormats/SiPixelObjects/interface/SiPixelGainForHLTonGPU.h"
 #include "HeterogeneousCore/CUDAUtilities/interface/cuda_assert.h"
 
-#include "gpuClusteringConstants.h"
+#include "gpuClusteringConstants.h" 
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
 
 namespace gpuCalibPixel {
 
@@ -18,7 +19,11 @@ namespace gpuCalibPixel {
   constexpr float VCaltoElectronOffset = -60;      // L2-4: -60 +- 130
   constexpr float VCaltoElectronOffset_L1 = -670;  // L1:   -670 +- 220
 
-  __global__ void calibDigis(uint16_t* id,
+  struct calibDigis {
+    template <typename T_Acc>
+    ALPAKA_FN_ACC
+    void operator()(T_Acc const& acc,
+                             uint16_t* id,
                              uint16_t const* __restrict__ x,
                              uint16_t const* __restrict__ y,
                              uint16_t* adc,
@@ -27,13 +32,13 @@ namespace gpuCalibPixel {
                              uint32_t* __restrict__ moduleStart,        // just to zero first
                              uint32_t* __restrict__ nClustersInModule,  // just to zero them
                              uint32_t* __restrict__ clusModuleStart     // just to zero first
-  ) {
+  ) const {
     int first = blockDim.x * blockIdx.x + threadIdx.x;
 
     // zero for next kernels...
     if (0 == first)
       clusModuleStart[0] = moduleStart[0] = 0;
-    for (int i = first; i < gpuClustering::MaxNumModules; i += gridDim.x * blockDim.x) {
+    for (uint32_t i = first; i < gpuClustering::MaxNumModules; i += gridDim.x * blockDim.x) {
       nClustersInModule[i] = 0;
     }
 
@@ -62,6 +67,7 @@ namespace gpuCalibPixel {
       }
     }
   }
+  };
 }  // namespace gpuCalibPixel
 
 #endif  // RecoLocalTracker_SiPixelClusterizer_plugins_gpuCalibPixel_h
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClusterChargeCut.h b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClusterChargeCut.h
index b81752cf282..32be7edfb86 100644
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClusterChargeCut.h
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClusterChargeCut.h
@@ -7,18 +7,23 @@
 #include "HeterogeneousCore/CUDAUtilities/interface/cuda_assert.h"
 #include "HeterogeneousCore/CUDAUtilities/interface/prefixScan.h"
 
+#include "RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h"
+
 #include "gpuClusteringConstants.h"
 
 namespace gpuClustering {
 
-  __global__ void clusterChargeCut(
+  struct clusterChargeCut {
+    template <typename T_Acc>
+    ALPAKA_FN_ACC
+    void operator()(T_Acc const& acc,
       uint16_t* __restrict__ id,                 // module id of each pixel (modified if bad cluster)
       uint16_t const* __restrict__ adc,          //  charge of each pixel
       uint32_t const* __restrict__ moduleStart,  // index of the first pixel of each module
       uint32_t* __restrict__ nClustersInModule,  // modified: number of clusters found in each module
       uint32_t const* __restrict__ moduleId,     // module id of each module
       int32_t* __restrict__ clusterId,           // modified: cluster id of each pixel
-      uint32_t numElements) {
+      uint32_t numElements) const {
     if (blockIdx.x >= moduleStart[0])
       return;
 
@@ -52,7 +57,7 @@ namespace gpuClustering {
         continue;  // not valid
       if (id[i] != thisModuleId)
         break;  // end of module
-      atomicAdd(&charge[clusterId[i]], adc[i]);
+      atomicAdd(&charge[clusterId[i]], (int) adc[i]); //requires an int as 2 arg ??
     }
     __syncthreads();
 
@@ -97,6 +102,7 @@ namespace gpuClustering {
 
     //done
   }
+  };
 
 }  // namespace gpuClustering
 
diff --git a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClustering.h b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClustering.h
index b610d02ba7c..dcf8d6676d7 100644
--- a/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClustering.h
+++ b/RecoLocalTracker/SiPixelClusterizer/plugins/gpuClustering.h
@@ -12,39 +12,45 @@
 
 namespace gpuClustering {
 
-  __global__ void countModules(uint16_t const* __restrict__ id,
-                               uint32_t* __restrict__ moduleStart,
-                               int32_t* __restrict__ clusterId,
-                               int numElements) {
-    int first = blockDim.x * blockIdx.x + threadIdx.x;
-    for (int i = first; i < numElements; i += gridDim.x * blockDim.x) {
-      clusterId[i] = i;
-      if (InvId == id[i])
-        continue;
-      auto j = i - 1;
-      while (j >= 0 and id[j] == InvId)
-        --j;
-      if (j < 0 or id[j] != id[i]) {
-        // boundary...
-        auto loc = atomicInc(moduleStart, MaxNumModules);
-        moduleStart[loc + 1] = i;
+  struct countModules {
+    template< typename T_Acc >
+    ALPAKA_FN_ACC
+    void operator()(T_Acc const& acc,
+                    uint16_t const* __restrict__ id,
+                    uint32_t* __restrict__ moduleStart,
+                    int32_t* __restrict__ clusterId,
+                    int numElements) const {
+      int first = blockDim.x * blockIdx.x + threadIdx.x;
+      for (int i = first; i < numElements; i += gridDim.x * blockDim.x) {
+        clusterId[i] = i;
+        if (InvId == id[i])
+          continue;
+        auto j = i - 1;
+        while (j >= 0 and id[j] == InvId)
+          --j;
+        if (j < 0 or id[j] != id[i]) {
+          // boundary...
+          auto loc = atomicInc(moduleStart, MaxNumModules);
+          moduleStart[loc + 1] = i;
+        }
       }
     }
-  }
+  };
 
-  __global__
-      //  __launch_bounds__(256,4)
-      void
-      findClus(uint16_t const* __restrict__ id,           // module id of each pixel
-               uint16_t const* __restrict__ x,            // local coordinates of each pixel
-               uint16_t const* __restrict__ y,            //
-               uint32_t const* __restrict__ moduleStart,  // index of the first pixel of each module
-               uint32_t* __restrict__ nClustersInModule,  // output: number of clusters found in each module
-               uint32_t* __restrict__ moduleId,           // output: module id of each module
-               int32_t* __restrict__ clusterId,           // output: cluster id of each pixel
-               int numElements) {
-    if (blockIdx.x >= moduleStart[0])
-      return;
+  struct findClus {
+    template< typename T_Acc >
+    ALPAKA_FN_ACC
+    void operator()(T_Acc const& acc,
+                    uint16_t const* __restrict__ id,           // module id of each pixel
+                    uint16_t const* __restrict__ x,            // local coordinates of each pixel
+                    uint16_t const* __restrict__ y,            //
+                    uint32_t const* __restrict__ moduleStart,  // index of the first pixel of each module
+                    uint32_t* __restrict__ nClustersInModule,  // output: number of clusters found in each module
+                    uint32_t* __restrict__ moduleId,           // output: module id of each module
+                    int32_t* __restrict__ clusterId,           // output: cluster id of each pixel
+                    int numElements) const {
+      if (blockIdx.x >= moduleStart[0])
+        return;
 
     auto firstPixel = moduleStart[1 + blockIdx.x];
     auto thisModuleId = id[firstPixel];
@@ -278,6 +284,7 @@ namespace gpuClustering {
 #endif
     }
   }
+  };
 
 }  // namespace gpuClustering
 
diff --git a/RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h b/RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h
index b58b10cf581..00de51afd0a 100644
--- a/RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h
+++ b/RecoPixelVertexing/PixelTrackFitting/test/cms_cupla.h
@@ -6,6 +6,13 @@
 #else
 #include <cupla/standalone/CpuSerial.hpp>
 #endif
-#include <cuda_to_cupla.hpp>
+//#include <cuda_to_cupla.hpp> //already included by one of the above includes
+
+void throw_if_error(cudaError_t success) {
+    if (success!=cudaSuccess) {
+        throw std::runtime_error(cudaGetErrorString(success));
+    }
+    return;
+}
 
 #endif
